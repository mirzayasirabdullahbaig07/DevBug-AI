# -*- coding: utf-8 -*-
"""Bug.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1yUS_Ec28l5zYuoshyJ7sXgMsUwKP2mrq
"""

# ================================
# STEP 1: Imports & Configuration
# ================================

import pandas as pd
import numpy as np

pd.set_option("display.max_columns", None)
pd.set_option("display.max_colwidth", 120)

RANDOM_STATE = 42

# ================================
# STEP 1: Load Dataset
# ================================

df = pd.read_csv(
    "bug_dataset_50k.csv",
    on_bad_lines="skip"
)

print("Dataset shape:", df.shape)
df.head()

# ================================
# STEP 1: Schema Validation
# ================================

EXPECTED_COLUMNS = {
    "title",
    "description",
    "severity",
    "environment",
    "tech_stack",
    "bug_domain",
    "error_code",
    "bug_category",
    "developer_role"
}

missing = EXPECTED_COLUMNS - set(df.columns)
extra = set(df.columns) - EXPECTED_COLUMNS

print("Missing columns:", missing)
print("Extra columns:", extra)

# ================================
# STEP 1: Data Integrity Checks
# ================================

print("Null counts:")
display(df.isnull().sum())

print("\nUnique values check:")
print("Severity:", df["severity"].unique())
print("Environment:", df["environment"].unique())
print("Bug Categories:", df["bug_category"].nunique())
print("Developer Roles:", df["developer_role"].nunique())

# ================================
# STEP 1: Label Distribution
# ================================

print("Bug category distribution:")
display(df["bug_category"].value_counts())

print("\nDeveloper role distribution:")
display(df["developer_role"].value_counts())

# ================================
# STEP 2: Select Relevant Columns
# ================================

KEEP_COLUMNS = [
    "title",
    "description",
    "severity",
    "environment",
    "tech_stack",
    "bug_domain",
    "error_code",
    "bug_category",
    "developer_role"
]

df = df[KEEP_COLUMNS].copy()

df.head()

# ================================
# STEP 2: Clean Text Fields
# ================================

def clean_text(text):
    if pd.isna(text):
        return ""
    text = str(text).lower().strip()
    return text

df["title"] = df["title"].apply(clean_text)
df["description"] = df["description"].apply(clean_text)

# Combine text (THIS is the only text field we will use)
df["text"] = df["title"] + " " + df["description"]

# Drop very short or empty text
df = df[df["text"].str.len() > 30]

print("Shape after text cleaning:", df.shape)

# ================================
# STEP 2: Metadata Cleaning
# ================================

df["severity"] = df["severity"].fillna("Medium")
df["environment"] = df["environment"].fillna("Development")
df["tech_stack"] = df["tech_stack"].fillna("Unknown")
df["bug_domain"] = df["bug_domain"].fillna("Unknown")
df["error_code"] = df["error_code"].fillna(0).astype(int)

df.isnull().sum()

# ================================
# STEP 2: Lock Controlled Categories
# ================================

ALLOWED_SEVERITY = {"Low", "Medium", "High", "Critical"}
ALLOWED_ENV = {"Development", "Staging", "Production"}

df = df[df["severity"].isin(ALLOWED_SEVERITY)]
df = df[df["environment"].isin(ALLOWED_ENV)]

print("Final cleaned shape:", df.shape)

# ================================
# STEP 3: Encode Controlled Metadata
# ================================

SEVERITY_MAP = {"Low": 0, "Medium": 1, "High": 2, "Critical": 3}
ENV_MAP = {"Development": 0, "Staging": 1, "Production": 2}

df["severity_code"] = df["severity"].map(SEVERITY_MAP)
df["environment_code"] = df["environment"].map(ENV_MAP)

df[["severity", "severity_code", "environment", "environment_code"]].head()

# ================================
# STEP 3: Freeze Open Categories
# ================================

TOP_TECH_STACKS = df["tech_stack"].value_counts().nlargest(20).index
TOP_DOMAINS = df["bug_domain"].value_counts().nlargest(20).index

df["tech_stack_clean"] = df["tech_stack"].where(
    df["tech_stack"].isin(TOP_TECH_STACKS), "Other"
)

df["bug_domain_clean"] = df["bug_domain"].where(
    df["bug_domain"].isin(TOP_DOMAINS), "Other"
)

df["tech_stack_clean"].value_counts().head()

from sklearn.preprocessing import LabelEncoder

tech_encoder = LabelEncoder()
domain_encoder = LabelEncoder()
dev_encoder = LabelEncoder()
bug_encoder = LabelEncoder()

df["tech_code"] = tech_encoder.fit_transform(df["tech_stack_clean"])
df["domain_code"] = domain_encoder.fit_transform(df["bug_domain_clean"])
df["dev_label"] = dev_encoder.fit_transform(df["developer_role"])
df["bug_label"] = bug_encoder.fit_transform(df["bug_category"])

# ================================
# STEP 3: Bug Text Features
# ================================

from sklearn.feature_extraction.text import TfidfVectorizer

tfidf = TfidfVectorizer(
    max_features=30000,
    ngram_range=(1, 2),
    stop_words="english",
    min_df=5
)

X_bug_text = tfidf.fit_transform(df["text"])
y_bug = df["bug_label"]

X_bug_text.shape

# ================================
# STEP 3: Text Embeddings
# ================================

from sentence_transformers import SentenceTransformer

embed_model = SentenceTransformer("all-MiniLM-L6-v2")

text_embeddings = embed_model.encode(
    df["text"].tolist(),
    show_progress_bar=True
)

text_embeddings.shape  # (n_samples, 384)

# ================================
# STEP 3: Developer Feature Matrix
# ================================

import numpy as np

X_dev = np.hstack([
    text_embeddings,
    df[[
        "severity_code",
        "environment_code",
        "error_code",
        "tech_code",
        "domain_code",
        "bug_label"
    ]].values
])

y_dev = df["dev_label"]

print("X_dev shape:", X_dev.shape)
print("y_dev shape:", y_dev.shape)

# ================================
# STEP 4: Train/Test Split (Bug Model)
# ================================

from sklearn.model_selection import train_test_split

X_bug_train, X_bug_val, y_bug_train, y_bug_val = train_test_split(
    X_bug_text,
    y_bug,
    test_size=0.2,
    random_state=42,
    stratify=y_bug
)

X_bug_train.shape, X_bug_val.shape

# ================================
# STEP 4: Train Bug Classifier
# ================================

from sklearn.linear_model import LogisticRegression

bug_model = LogisticRegression(
    max_iter=2000,
    n_jobs=-1,
    class_weight="balanced"
)

bug_model.fit(X_bug_train, y_bug_train)

# ================================
# STEP 4: Evaluation
# ================================

from sklearn.metrics import classification_report, accuracy_score

y_bug_pred = bug_model.predict(X_bug_val)

print("Accuracy:", accuracy_score(y_bug_val, y_bug_pred))
print(classification_report(
    y_bug_val,
    y_bug_pred,
    target_names=bug_encoder.classes_
))

# ================================
# STEP 4: Save Bug Model
# ================================

import joblib
import os

os.makedirs("models", exist_ok=True)

joblib.dump(bug_model, "models/bug_classifier.pkl")
joblib.dump(tfidf, "models/tfidf_vectorizer.pkl")
joblib.dump(bug_encoder, "models/bug_label_encoder.pkl")

print("Bug classification model saved.")

# ================================
# STEP 5: Developer Feature Matrix
# ================================

# X_dev already created in Step 3
# Shape should be (50000, 390)

X_dev.shape

# ================================
# STEP 5: Train/Test Split (Developer)
# ================================

from sklearn.model_selection import train_test_split

X_dev_train, X_dev_val, y_dev_train, y_dev_val = train_test_split(
    X_dev,
    y_dev,
    test_size=0.2,
    random_state=42,
    stratify=y_dev
)

X_dev_train.shape, X_dev_val.shape

# ================================
# STEP 5: Train Developer Model
# ================================

from lightgbm import LGBMClassifier

dev_model = LGBMClassifier(
    n_estimators=300,
    learning_rate=0.05,
    max_depth=-1,
    num_leaves=64,
    subsample=0.9,
    colsample_bytree=0.9,
    random_state=42,
    n_jobs=-1
)

dev_model.fit(X_dev_train, y_dev_train)

# ================================
# STEP 5: Evaluation
# ================================

from sklearn.metrics import accuracy_score, classification_report

y_dev_pred = dev_model.predict(X_dev_val)

print("Accuracy:", accuracy_score(y_dev_val, y_dev_pred))
print(classification_report(
    y_dev_val,
    y_dev_pred,
    target_names=dev_encoder.classes_
))

# ================================
# STEP 5: Save Developer Model
# ================================

joblib.dump(dev_model, "models/dev_model.pkl")
joblib.dump(dev_encoder, "models/dev_encoder.pkl")
joblib.dump(tech_encoder, "models/tech_encoder.pkl")
joblib.dump(domain_encoder, "models/domain_encoder.pkl")

print("Developer recommendation model saved.")

# ================================
# STEP 6: Unified Inference Pipeline
# ================================

def predict_bug_and_developer(
    bug_text: str,
    severity: str,
    environment: str,
    error_code: int,
    tech_stack: str,
    bug_domain: str
):
    # ---------- Encode metadata ----------
    severity_map = {"low": 0, "medium": 1, "high": 2, "critical": 3}
    env_map = {"development": 0, "staging": 1, "production": 2}

    severity_code = severity_map[severity.lower()]
    env_code = env_map[environment.lower()]

    tech_code = tech_encoder.transform([tech_stack])[0]
    domain_code = domain_encoder.transform([bug_domain])[0]

    # ---------- Bug category prediction ----------
    bug_vec = tfidf.transform([bug_text.lower()])
    bug_label = bug_model.predict(bug_vec)[0]
    bug_category = bug_encoder.inverse_transform([bug_label])[0]

    # ---------- Developer prediction ----------
    embedding = embed_model.encode([bug_text])

    X_dev_input = np.hstack([
        embedding,
        [[severity_code, env_code, error_code, tech_code, domain_code, bug_label]]
    ])

    assert X_dev_input.shape[1] == dev_model.n_features_in_

    dev_label = dev_model.predict(X_dev_input)[0]
    developer = dev_encoder.inverse_transform([dev_label])[0]

    return bug_category, developer

predict_bug_and_developer(
    bug_text="User login fails with 401 error after token expiry",
    severity="High",
    environment="Production",
    error_code=401,
    tech_stack="Flask",
    bug_domain="Backend Systems"
)

import pandas as pd

# ----------------------------

# Excel file with your 10 bug inputs

# ----------------------------

data = [
["User cannot login after password reset", "High", "Production", 401, "Flask", "Backend Systems"],
["Page layout breaks on mobile devices", "Medium", "Staging", 0, "React", "Frontend"],
["API returns 500 on large payloads", "High", "Production", 500, "Django", "API"],
["Memory usage spikes after cron job", "Medium", "Development", 0, "Python", "Data"],
["CI pipeline fails when building Docker image", "Medium", "Staging", 1, "GitHub Actions", "CI/CD"],
["User cannot upload files larger than 5MB", "Low", "Production", 413, "Angular", "Frontend"],
["Database deadlock occurs under heavy load", "Critical", "Production", 1205, "PostgreSQL", "Database"],
["Cloud config not applied to new VM instances", "Medium", "Production", 404, "AWS", "Cloud Configuration"],
["Logging not appearing in production servers", "Low", "Production", 503, "Flask", "Logging"],
["Frontend routing redirects incorrectly after login", "Medium", "Staging", 302, "Vue.js", "Frontend Routing"]
]

columns = ["Bug Text", "Severity", "Environment", "Error Code", "Tech Stack", "Bug Domain"]

# Create DataFrame

df = pd.DataFrame(data, columns=columns)

# Save to Excel

df.to_excel("bug_inputs.xlsx", index=False)
print("Excel file 'bug_inputs.xlsx' created. You can download it.")